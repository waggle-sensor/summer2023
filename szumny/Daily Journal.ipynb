{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 1 (05/30-06/2)   \n",
    "                          \n",
    "Day 1 (Tuesday, May 30th, 2023): Today was my first day working at Argonne. I was pretty nervous to start but after going through my day I calmed down. The day started off with attending orientation from 9-12, and then lunch from 12-1. It was then time to report to my workstation and meet with my mentors and supervisor. It was really great to meet them, and they gave me a rundown of my project and what to expect from it. I didn’t do much actual work, but I spent the rest of the day completing all my required training, and also set up my computer to be able to code.\n",
    "\n",
    "Day 2 (Wednesday, May 31st, 2023): Today started off by once again attending orientation from 9-12. I went to lunch yet again, and at 1 reported back to my workstation. I had to complete the rest of my training as they had added some for me to do. I then began to start working on my project. My first task was to convert all of the xml files from the given data into one large csv file, with information such as the filename, the folder, and the coordinates of the smoke. I worked on this all day, but got stuck on figuring out how to get the coordinates into their correct spots. The reason for this was because the direct child of the object in the xml file was not multiple x’s and y’s, but one large text which I would have to split up in order to get the respective x’s and y’s.\n",
    "\n",
    "Day 3 (Thursday, June 1st, 2023): Today was my first full day at work. I got to my station at 8:30, and began by checking all my emails and making sure I'm up to date with everything. I then created my personal introduction chart, and then I began trying to crack down on the problem. I managed to figure out how to successfully get the maximum and minimum of the x’s and y’s into their respective dataframe locations. I did this by looping through the child object and comparing them to preset variables in order to get the absolute max and absolute min. After I completed this, I went over to the weekly meeting at 10:00, in which we all introduced ourselves, and asked any question that we may have had. I created a different function, which would iterate through every folder and file of xml’s from the data, run each of those xml files through the single xml to csv file function which I created. In the code it would create each individual dataframe, and then concat or combine them all into one large dataframe. I was very proud after I completed this step, because it felt like it took forever. After this I went to lunch until 1:00. Once I came back, I started trying to figure out how to crop each image. My next step was to take every instance of smoke from the csv file I created, and crop it at the given points to only have pictures of the actual smoke. Bhupendra told me to first only crop one individual image, in order to see how it crops. I used the Pillow Image functions in order to do this, but once I cropped the image I realized that something was wrong. It didn’t crop only the smoke and instead cropped the image randomly. I thought about why this may be and after looking through my code I realized that when I actually converted the xml to the df, I was grabbing the points of the image, and not the points of the boundary boxes of the smoke. I went back to fix my mistake, and once I had gone through my other code to fix and other errors that arose, the single image crop worked perfectly. I then wrote a function which would take the csv file of size 9,550, and crop each image in the dataset to their smoke coordinates. This function was much easier to create, but when I was running it (which took a very long time because I was saving 9,500 cropped images into a separate folder on my computer), the function kept crashing at one given moment. After looking at the error message which said the given directory was not found, I double checked and found that there was actually a typo in the xml file for the folder name, where it contained a dash and not an underscore. I added a condition in the code where if that specific folder was found, to change the name to its correct name, which worked. This error happened a few times while running the code, but I managed to correct all of the discrepancies and the function finished running. I now have a complete file with all images of the smoke which were cropped.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 4 (Friday, June 2nd, 2023): I first began by trying to figure out how to resize my images. I had to think about this for a while because this would be very important later on. I did some analyzing of the image sizes from my cropped images, and found that a majority of them were at least 100 by 100. So I plan on going in that direction. I started exploring a way to split the images further, specifically by rgb color ratios, but later after talking with bhupendra, I will do it simpler now and can go down that path later on. My next goal was to get about the same amount of sky images and ground images as my smoke imagees (about 9000). I was pretty stuck on this, and was trying out many things. I first tried to reverse crop the images, to remove the smoke and leave everything else. I worked on this and when I was done, I realized there was too much error, where a lot of smoke was still ending up in the image. I decided to go down a different road, and to figure out how to crop the images better. I created a program, which would check if the beginning of the smoke was before or after the middle of that image. If it was before, I would crop the full image from the max x of the smoke to the full size of the image, and if it was after the middle, I would crop the image from 0 to the min x of the smoke. I kept y as the full image, as I want to be able to use the sky and the ground. I tested it, but having it start right at the start or end of smoke also had a little bit of smoke poking through, so I then respectively pushed it back by about 250 pixels, which seemed to do the trick. At this point I was cropping the images well to not have smoke, but now I had to seperate the sky and the ground. I once again cropped the image I got in the same for loop, and changed the y this time, while keeping the full x of the cropped image. I changed y by a few different values with trial and error, but then found that 1050, was a pretty good height to seperate the ground and the sky. Once I ran this code, I was left with 2 new folders containing either sky or ground pictures. These pictures were very large, and I recieved advice to split the images up to have even more images. This was a tough one, but I found a really good function called image_slicer, which could slice an image and save the new images automatically with a given input of the amount of slices. I also did a bit of trial and error with the amount of slices, but I found that 9 keeps the images mostly above 100 by 100, and had pretty solid results. I ran this program and it worked perfectly as it saved about 9,000 new sliced images into new files, ground slices and sky slices respectively. This now leaves me with a folder of 9,000 smoke images, and 9,000 sky and ground images as well. This will allow me to obtain better results when I begin creating a model. Starting to create this model will be my first task starting next week."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 2 (06/05 - 06/09)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 1 (Monday, June 6th, 2023): Since I managed to get all of the necessary pictures last Friday for the dataset, today I was able to actually go and begin starting to train the VGG16 model with my dataset. First, I had to resize all of the images to be able to be used by the VGG16 model. This size is 224x224, and currently my images were all around the place. Also, in order to convert the images into the data that the dataset could use, I needed to put them in a new folder, and the folder names were actually the classes. I did this all in one step, by creating code which would take all of the pictures, and resize them and put those copies into the new folders. The folders actually also had to be seperated by training and testing data, so I incorporated a random event of 80% and 20%, and if it was 80% I would resize the image and send it to the training file, and otherwise send it to the testing file. This way the pictures would be randomly spread out, leading to better results. I found a few different websites explaining how to use the VGG16 model, so I began to try and implement it. There ws a lot of trial and error, as at first my images weren't being converted, and then later the model was not accepting my data. In the end I managed to have everything work, and began to train the model. The issue with this, is that it takes incredibly long to do so. My first attempt I waited for a very long time, just to realize that it was only for the first epoch, and there were 99 more to go. This would not work, so I managed to reduce the computational time by almost 40 minutes for each epoch, and also reduced the amount of steps per epoch. Then there was more trial and error. The model would refuse to run, and it turned out it was an issue with the way I installed keras. Once I fixed that, my checkpoint was not working. The checkpoint is very imporatnt, as it allows you to run the training for a very long time, and stop and continue whenever you'd like, as it takes the most accurate epoch after every one, and saves it to a file on my computer. I finally figured out how to save it after each epoch. Now I am just letting it train, and hopefully tomorrow I will be able to test the data on my first attempt at the model. Currently it is at about a 65% accuracy, which is incredibly good for the first attempt. Only one issue exists and it is that when I try and start the model at the last best saved weights, it has 19 insteead of 16 layers, which I will have to figure out tomorrow.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 2 (Tuesday, June 7th, 2023): I tried to figure out how to load the data, but nothing seemed to work. I ran it a few times, but it also was a very big program, so it also kept on crashing. I tried everything, but Seongha recommended I use the PyTorch library instead of TensorFlow. She said it was a little easier to use, and see where it goes wrong. So I found a few PyTorch VGG16 models online, and tried to implement them. This forced me to have to switch up the way I stored my files, in order for the model to be easily able to read the data. I basically just added another folder with validation data, and did this on a 70, 20, 10 split between training, validation, and testing, respectively. After that I was able to start writing the model. Once I was done, I could already tell that this new model worked much better than before. A new issue ended up arising, where my computer does not have a GPU and low memory, so I am not able to train my model on it. Sean recommended that I use Google Colab, as the base free version allows for 12 GB of memory, and a decent GPU. I am now currently working on transferring the model over to there, and my data, in order to train it on that instead. Hopefully tomorrow I will be able to finally complete the training of the model, and be able to test my first VGG16 model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 3 (Wednesday, June 8th, 2023): After my meeting in the morning, I started to upload my data to Google Colab. This ended up taking a very long time, because of how many images were in the dataset. What I ended up figuring out, is that it is much faster if I compress it to a zip file, upload it to google drive, and then access it from google colab. This ended up working very well, and I was able to unzip the file on Colab, for use. I started to copy over all the code I wrote from VSCode into Colab, and had to just make a few changes as I was not able to access my files from my computer anymore. I then quickly realized while trying to train the data, that I forgot to enable the gpu runtime, and I was running it the same as I was before. When I switched the runtime, it actually deleted my dataset, but this was not a hard fix, as I just unzipped the zip file yet again. Once I started to run the code to train the model, I was genuinely amazed at how fast it was. My old computer used to get through 100 training batches in about 10 minutes, and Colab was getting through them in about 30 seconds or less. Once again thanks to Sean for giving this suggestion. When my model finally trained, I received oddly great reesults. The accuracy of the model was about 94 - 98 percent, which seems too good to be true on my first attempt. Then after testing the model, It also did really well on the predictions, with images that the model has never seen before, with an accuracy of 99.1%. I'm not sure if my model is overfit or not, but I will be checking for that tomorrow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 4 (Thursday, June 9th, 2023): I presented my work so far, and got some good feedbaack. Seongha and I believe that my data is possibly too simple, and it's too easy for the mode to pickup, which wouldn't work out with a real world scenario input. So after the meeting, I started to reconstruct the data, the smoke stayed almost exactly the same, I just renamed the pictures to their correlating images. Now what I did differently, is the way I split the ground and sky. This time I am adding horizon, and using every single part of the pictures except for the smoke. I created a program which takes the picture, crops the smoke out and then saves the pictures. Then I created another program which takes those new pictures and splits them 3 ways, for the sky, horizon, and ground. There were about 8,300 pictures, so I have 8,300 pictures of each class. I also resized them in the same program, and accounted for smoke that may still be in the image. I hope I will have the cloud images by tomorrow, to add to the dataset as well. Either way I will be training a new VGG16 model, to see if this seems more accurate now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 5 (Friday, June 10th, 2023): After some thought last night, thee new dataset I created was not good. There are many instances of each fire, but the setting does not change throughout those instances, only the smoke does. The ground and sky is just the same for each, so the dataset I created yesterday had way too many duplicates. When I came in, I began to create the program for the new dataset. Smoke is not going to change as I believe it is already good enough. I decided to first do 3 rows and 4 columns. I compiled all the cropped and resized images, but it was far too little images. I kept thinking about it, and actually decided in the end to do 5 rows, and 6 columns. This way each full image(without the smoke) can be split into even tiles, which are then resized to 224x224. So this means that for every one setting, I get 12 sky images, 6 horizon images, and 12 ground images. There really isn't a good way to get 12 horizon images, as the key part of the image is the seperation of the ground and sky, so it should be alright with 6. Then once I had this dataset, I felt really good about how it turned out, so I went back to train the model. After trainining, I actually evaluated the model at a 90% accuracy, which is very good. It makes much more sense than the 98% model, and it is definitely because of the dataset improvement. After I did this model, I tested the model on predicting not everything, but just other and smoke. My first try training I once again got an oddly great model accuracy, but after further investigation I found that somehow all of my labeled sky images were actually ground images, so the data was heavily biased. I went back and fixed my mistake, and recieved a 96% accuracy on the testing data. Once again, I am not sure if my data is still too simple, but this can come to the test when I will test the model with random node images. My next steps include uploading my dataset to the lcrc, and also to test my dataset on the Smokeynet architecture from UCSD, to see if my data really is too simple, or if the simple VGG16 is simply just better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
